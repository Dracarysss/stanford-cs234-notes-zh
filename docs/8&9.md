# Lecture 8 & 9 Policy Gradient

# 课时8&9 策略梯度 2018.03.20

## 1. 策略搜索介绍（Introduction to Policy Search）

到目前为止，为学习策略，我们主要研究了基于值的（value-based）方法，在这类方法中，我们以参数 $\theta$ 的函数来近似最优状态值函数或状态-行为值函数：
$$
V_{\theta}(s)\approx V^{\pi}(s)，
$$

$$
Q_{\theta}(s,a)\approx Q^{\pi}(s,a)，
$$
然后通过 $V_{\theta}$ 或 $Q_{\theta}$ 得到策略，即 $\epsilon$-贪婪策略。我们也可以使用基于策略的（policy-based）方法来直接将策略参数化：
$$
\pi_{\theta}(a|s)=\mathbb{P}[a|s;\theta]。
$$

在这种设定中，我们的目标是根据最大的值函数 $V^{\pi}$ 来直接找到策略，而不是先找到基于最优策略的值函数，再从中得到策略。我们将考虑参数化的随机策略，而不是那种从状态到动作的查表式策略。找到一个好的策略需要以下两点：

$\bullet$ 好的策略参数化形式：我们的函数近似和状态/动作表达形式必须是充分的；

$\bullet$ 有效的搜索：我们必须能够为我们的策略函数近似找到好的参数。

基于策略的 RL 相比基于值的 RL 而言有一些优点：

$\bullet$ 更好的收敛性（见 Sutton and Barto 的第 13.3 章）；

$\bullet$ 高维或连续动作空间下的有效性，比如机器人，第 6.2 节介绍了处理连续动作空间的一种方法；

$\bullet$ 学习随机策略的能力（见后面部分）。

基于策略的 RL 方法的缺点是：

$\bullet$ 由于依赖于梯度下降，它们通常收敛于局部最优策略，而不是全局最优策略；

$\bullet$ 评估一个策略通常是数据效率低下或方差很大的。

## 2. 随机策略（Stochastic Policies）

本节我们将简要介绍两种环境，这两种环境中，随机策略优于任何的确定性策略。

### 2.1 石头-剪刀-布（Rock-paper-scissors）

作为一个相关的例子，在流行的剪刀-石头-布（和为零）游戏中，任何非均匀随机的策略：
$$
P(\text{rock}|s)=1/3
$$

$$
P(\text{scissors}|s)=1/3
$$

$$
P(\text{paper}|s)=1/3
$$
都可被利用。

### 2.2 网格世界（Aliased Gridworld）

# 图1

在图 1 的网格世界环境中，假设行为体可以向四个基本的方向移动，即动作空间为 $A={N,S,E,W}$。假设它只能感知当前位置周围的墙，具体而言，它观察每个方向的以下形式的特征：
$$
\phi(s)=\begin{bmatrix} \mathbf{1}(\text{wall to N}) \\ \cdots \\ \mathbf{1}(\text{wall to W}) \\ \end{bmatrix}。
$$

注意，它的观测结果并不能完全代表环境，因为它不能区分两个灰色的方块，这也意味着它的域不是马尔可夫的。因此，确定性策略必须要么学会在灰色方块中一直向左走，要么一直向右走。由于行为体可能陷在环境中的某个角落，所以这两种策略都不是最优的：

# 图2

而随机策略则可以学习在灰色的状态下随机选择一个方向，从而保证对于任何起始位置，它最终都能获得奖励。一般来说，随机策略有助于克服对抗或非平稳域（adversarial or non-stationary domain）以及状态表示为非马尔可夫的情况。

# 图3

# 3. 策略优化（Policy Optimization）

## 3.1 策略目标函数（Policy Objective Function）

定义了一个策略 $\pi_{theta}(a|s)$ 后，我们需要能够衡量它的表现，以便优化它。在片段式环境（episodic environment）中，一种度量量是策略的起始价值（start value of the policy），即起始状态的期望价值：
$$
J_1(\theta)=V^{\pi_{\theta}}(s_1)=\mathbb{E}_ {\pi_{\theta}}[v_1]。
$$

在连续环境中，我们可以使用策略的平均价值（average value of the policy）作为度量量，这里 $d^{\pi_{\theta}}(s)$ 为 $\pi_{\theta}$ 的稳态分布（stationary distribution）：
$$
J_{avV}(\theta)=\sum_s d^{\pi_{\theta}}(s)V^{\pi_{\theta}}(s)，
$$
也可以使用每时间步的平均奖励（average value per time-step）：
$$
J_{avR}(\theta)=\sum_s d^{\pi_{\theta}}(s) \sum_a \pi_{\theta}(a|s)R(s,a)。
$$

本课程中，我们讨论片段式的情况，但我们得到的所有结果都可以很容易地被推广到非片段式的情况。我们还将关注衰减因子 $\gamma=1$ 的情况，同样地，结果也可以很容易地被推广到一般的 $\gamma$。

## 3.2 优化方法（Optimization Methods）

利用目标函数，我们可以将基于策略的强化学习问题转化为优化问题。本课程中，我们关注梯度下降，因为最近这是基于策略的强化学习方法的最常用的优化方法。但一些无梯度优化方法（gradient-free optimization methods）也值得考虑，包括：

$\bullet$ 爬山（Hill climbing）

$\bullet$ ？？？（Simplex / amoeba / Nelder Mead）

$\bullet$ 遗传算法（Genetic algorithms）

$\bullet$ 交叉熵方法（Cross-entropy methods, CEM）

$\bullet$ 协方差矩阵自适应（Covariance matrix adaption, CMA）

$\bullet$ 进化策略（Evolution strategies）

与基于梯度的方法相比，这些方法的优点是不需要计算目标函数的梯度，这就允许了参数化策略可以是不可导的，而且通常也很容易并行化这些方法。无梯度方法通常是个有用的基线，有时候这些方法的表现出奇的好 [1]。然而，由于这些方法忽略了奖励的时间结构，即更新只考虑整个片段的总奖励，而不会将奖励分解为轨迹中的每个状态的奖励，因此它们通常不是数据高效的。