# Lecture 8 & 9 Policy Gradient

# 课时8&9 策略梯度 2018.03.20

## 1. 策略搜索介绍（Introduction to Policy Search）

到目前为止，为学习策略，我们主要研究了基于值的（value-based）方法，在这类方法中，我们以参数 $\theta$ 的函数来近似最优状态值函数或状态-行为值函数：
$$
V_{\theta}(s)\approx V^{\pi}(s)，
$$

$$
Q_{\theta}(s,a)\approx Q^{\pi}(s,a)，
$$
然后通过 $V_{\theta}$ 或 $Q_{\theta}$ 得到策略，即 $\epsilon$-贪婪策略。我们也可以使用基于策略的（policy-based）方法来直接将策略参数化：
$$
\pi_{theta}(a|s)=\mathbb{P}[a|s;\theta]。
$$