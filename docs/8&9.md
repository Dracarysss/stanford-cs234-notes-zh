# Lecture 8 & 9 Policy Gradient

# 课时8&9 策略梯度 2018.03.20

## 1. 策略搜索介绍（Introduction to Policy Search）

到目前为止，为学习策略，我们主要研究了基于值的（value-based）方法，在这类方法中，我们以参数 $\theta$ 的函数来近似最优状态值函数或状态-行为值函数：
$$
V_{\theta}(s)\approx V^{\pi}(s)，
$$

$$
Q_{\theta}(s,a)\approx Q^{\pi}(s,a)，
$$
然后通过 $V_{\theta}$ 或 $Q_{\theta}$ 得到策略，即 $\epsilon$-贪婪策略。我们也可以使用基于策略的（policy-based）方法来直接将策略参数化：
$$
\pi_{\theta}(a|s)=\mathbb{P}[a|s;\theta]。
$$

在这种设定中，我们的目标是根据最大的值函数 $V^{\pi}$ 来直接找到策略，而不是先找到基于最优策略的值函数，再从中得到策略。我们将考虑参数化的随机策略，而不是那种从状态到动作的查表式策略。找到一个好的策略需要以下两点：

$\bullet$ 好的策略参数化形式：我们的函数近似和状态/动作表达形式必须是充分的；

$\bullet$ 有效的搜索：我们必须能够为我们的策略函数近似找到好的参数。

基于策略的 RL 相比基于值的 RL 而言有一些优点：

$\bullet$ 更好的收敛性（见 Sutton and Barto 的第 13.3 章）；

$\bullet$ 高维或连续动作空间下的有效性，比如机器人，第 6.2 节介绍了处理连续动作空间的一种方法；

$\bullet$ 学习随机策略的能力（见后面部分）。

基于策略的 RL 方法的缺点是：

$\bullet$ 由于依赖于梯度下降，它们通常收敛于局部最优策略，而不是全局最优策略；

$\bullet$ 评估一个策略通常是数据效率低下或方差很大的。

## 2. 随机策略（Stochastic Policies）

本节我们将简要介绍两种环境，这两种环境中，随机策略优于任何的确定性策略。

### 2.1 石头-剪刀-布（Rock-paper-scissors）

作为一个相关的例子，在流行的剪刀-石头-布（和为零）游戏中，任何非均匀随机的策略：
$$
P(\text{rock}|s)=1/3
$$

$$
P(\text{scissors}|s)=1/3
$$

$$
P(\text{paper}|s)=1/3
$$
都可被利用。