# Lecture 7 Imitation Learning

# 课时7 模仿学习 2018.03.20

## 8. 介绍（Introduction）

强化学习中有一些必须克服的理论和实践上的障碍，包括优化、延迟的影响、如何探索以及如何归纳。然而，重要的是，我们希望能够处理上述所有困难，同时保证数据效率和计算效率。

我们将在课程后后面部分讨论有效探索的一般方法，这些方法能够处理一般的 MDPs。但是，如果我们知道问题的结构，或者我们有可以使用的外部知识，我们就可以更有效地探索。本次课我们讨论如何模仿和学习人类（专家）在任务上的行为。

## 9. 模仿学习（Imitation Learning）

过去我们的目标是从奖励中学习策略，这些奖励通常非常稀疏（sparse），例如，一个简单的奖励信号可能是行为体是否赢得了一场游戏。这种方法在数据便宜且易于收集的情况下是成功的，然而，当数据收集速度较慢、必须避免失败（如自动驾驶）或必须保证安全时，这种方法就不是可行的。

缓解稀疏奖励问题的一种途径是手动设计在时间上密集的奖励函数，然而，这种方法需要人来手工设计奖励函数，并记住所需的行为，因此，最好通过模仿执行相关任务的行为体来学习。

### 9.1 从示范中学习（Learning from Demonstration）

一般来说，专家们提供了一组演示轨迹（demonstration trajectories），这些轨迹是状态和动作的序列。更正式地，我们假设已知：

$\bullet$ 状态空间，动作空间；

$\bullet$ 状态转移模型 $P(s'|s,a)$；

$\bullet$ 奖励函数 $R$ 未知；

$\bullet$ 一个或多个演示 $(s_0,a_0,s_1,a_1,...)$，这里动作遵循策略 $\pi^{ast}$。

## 10. 行为克隆（Behavioral Cloning）