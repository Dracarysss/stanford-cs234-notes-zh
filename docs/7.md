# Lecture 7 Imitation Learning

# 课时7 模仿学习 2018.03.20

## 8. 介绍（Introduction）

强化学习中有一些必须克服的理论和实践上的障碍，包括优化、延迟的影响、如何探索以及如何归纳。然而，重要的是，我们希望能够处理上述所有困难，同时保证数据效率和计算效率。

我们将在课程后后面部分讨论有效探索的一般方法，这些方法能够处理一般的 MDPs。但是，如果我们知道问题的结构，或者我们有可以使用的外部知识，我们就可以更有效地探索。本次课我们讨论如何模仿和学习人类（专家）在任务上的行为。

## 9. 模仿学习（Imitation Learning）

过去我们的目标是从奖励中学习策略，这些奖励通常非常稀疏（sparse），例如，一个简单的奖励信号可能是行为体是否赢得了一场游戏。这种方法在数据便宜且易于收集的情况下是成功的，然而，当数据收集速度较慢、必须避免失败（如自动驾驶）或必须保证安全时，这种方法就不是可行的。

缓解稀疏奖励问题的一种途径是手动设计在时间上密集的奖励函数，然而，这种方法需要人来手工设计奖励函数，并记住所需的行为，因此，最好通过模仿执行相关任务的行为体来学习。

### 9.1 从示范中学习（Learning from Demonstration）

一般来说，专家们提供了一组演示轨迹（demonstration trajectories），这些轨迹是状态和动作的序列。更正式地，我们假设已知：

$\bullet$ 状态空间，动作空间；

$\bullet$ 状态转移模型 $P(s'|s,a)$；

$\bullet$ 奖励函数 $R$ 未知；

$\bullet$ 一个或多个演示 $(s_0,a_0,s_1,a_1,...)$，这里动作遵循策略 $\pi^{\ast}$。

## 10. 行为克隆（Behavioral Cloning）

<div align=center>
我们能否通过有监督学习来学习策略？
</div>
<br/>

行为克隆是为了通过有监督学习去学习策略。具体来说，我们固定一个策略的类，目的是在已知数据 ${(s_0,a_0),(s_1,a_1),...}$ 时，学习一个将状态映射到动作的策略。一个值得注意的例子是 ALVINN，它学会了将传感器输入映射到转向角。

这种方法的一个挑战是状态空间的数据不是 i.i.d.（独立同分布，independent and identically distributed）的，i.i.d. 的假设是有监督学习文献和理论的标准。然而，在 RL 环境中，错误是复杂的；这些错误会随着事件过程不断累积。用于被学习的策略的数据会紧紧围绕专家的轨迹，如果犯了一个错误，而且这个错误使得行为体处于专家没有访问过的状态，那么行为体就没有可以从中学习策略的数据。在这种情况下，与标准 RL 中的线性标度相反，误差在片段中呈四次标度（the error scales quadratically）。

### 10.1 DAGGER：数据聚集（DAGGER: Dataset Aggregation）

数据聚集（DAGGER，算法 1，[1]）的目的是通过为新访问的状态添加数据来缓解错误的复杂化问题。与假设存在预先定义好的专家演示相反，我们假设我们可以通过专家来生成更多的数据。当然，这么做的局限之处在于，专家必须能够提供标签，有时还应该是实时的。

# 算法1

## 11. 逆强化学习（Inverse Reinforcement Learning, IRL）

<div align=center>
我们能否复原奖励函数 $R$？
</div>
<br/>

在逆强化学习中（也被称为逆最优控制（inverse optimal control）），我们的目标是基于专家演示来学习奖励函数（未知）。如果不假设演示的最优性，这个问题是很难解决的，因为任何奖励函数都可能产生观测到的轨迹。

### 11.1 ？？？？(Linear Feature Reward Inverse RL)

我们考虑表示为特征的线性组合的奖励：
$$
R(s)=\omega^{\text{T}}x(s)，
\tag{1}
$$

这里 $\omega\in \mathbb{R}^n$，$x:S\rightarrow \mathbb{R}^n$。在这种情况下，IRL 问题是给定一组演示时，确定权重向量 $\omega$。由此生成的关于策略 $\pi$ 的价值函数可以表示为：
$$
V^{\pi}(s)=\mathbb{E}_ {\pi}[\sum_{t=0}^{\infty}\gamma^tR(s_t)|s_0=s]
\tag{2}
$$

$$
=\mathbb{E}_ {\pi}[\sum_{t=0}^{\infty}\gamma^t\omega^{\text{T}}x(s_t)|s_0=s]
\tag{3}
$$

$$
=\omega^{\text{T}}\mathbb{E}_ {\pi}[\sum_{t=0}^{\infty}\gamma^tx(s_t)|s_0=s]
\tag{4}
$$

$$
=\omega^{\text{T}}\mu(\pi)，
\tag{5}
$$

这里 $\mu(\pi|s_0=s)\in\mathbb{R}^n$ 为基于策略 $\pi$ 的状态特征 $x(s)$ 的衰减加权和。注意：
$$
\mathbb{E}_ {\pi^{\ast}}[\sum_{t=0}^{\infty}\gamma^tR^{\ast}(s_t)|s_0=s]\geq \mathbb{E}_ {\pi}[\sum_{t=0}^{\infty}\gamma^tR^{\ast}(s_t)|s_0=s] \quad \forall\pi，
\tag{6}
$$

这里 $R^{\ast}$ 为最优奖励函数。因此，如果专家演示是最优的（即动作是基于最优策略生成的），那么我们找到满足下式的 $\omega^{\ast}$ 就能够确定 $\omega$：
$$
\omega^{\ast\text{T}}\mu(\pi^{\ast}|s_0=s) \geq \omega^{\ast\text{T}}\mu(\pi|s_0=s) \quad \forall\pi，\forall s，
\tag{7}
$$