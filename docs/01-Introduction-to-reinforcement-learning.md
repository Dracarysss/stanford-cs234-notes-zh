Lecture 1 Introduction to Reinforcement Learning

课时1 强化学习介绍 2018.03.20

1. 引言

在强化学习（Reinforcement Learning）中，我们考虑在没有明确的指导的情况下，学习如何通过经验（experience）去进行动作（action）的问题。强化学习行为体（agent）需要同它所处的环境（environment）交互，并从中学习如何最大化随时间累积的奖励（reward）。

可能是由于 Deep Q-Network 取得了巨大进步，近些年来强化学习越来越收到欢迎。人工智能的其他领域也正尝试通过借鉴与利用强化学习中的概念去取得成功。例如，AlphaGo 通过强化学习的方法在围棋领域达到了超过人类的表现，强化学习的概念在生成对抗网络（Generative Adversarial Networks, GAN）的训练过程中也得以应用。

很多人好奇强化学习与其他机器学习方式的不同之处。在有监督学习（Supervised Learning）中，已知包括了例子和标记的数据集，对于分类问题，例子对应的正确的标签已知；对于回归问题，例子对应的正确的输出已知。相反，无监督学习（Unsupervised Learning）指的是利用标记未知的数据集，找到数据中隐含的关系。在强化学习中，我们需要做出决策并比较可采取的动作，而不是做出预测。强化学习行为体可以和环境交互，并在每次交互中获得一些立即的、局部反馈信号，这些信号通常被称作奖励。但是，行为体不知道它采取的动作是否是它能够选择的“最好的“动作，它必须通过某种方式去学习通过选择一些动作来最大化长期的累积的奖励。因此，由于奖励信号提供的反馈很弱/不完整，我们可以认为强化学习介于有监督学习和无监督学习之间：有监督学习中的有标记的数据提供强有力的反馈，而无监督学习中没有标记或反馈。

强化学习引入了许多需要克服的困难，并且我们可能要在这些挑战中进行权衡。行为体必须能够优化它的动作以使其收到的奖励达到最大，但是，因为它需要通过与环境交互来学习，探索（exploration）也是需要的。这自然就导致了探索与利用（exploitation）的权衡，在这种情况下，行为体需要决定是寻找新的、更好的策略但要承担获得较低奖励的风险，还是利用它已经知道的策略。另一个我们需要面对的问题是，行为体是否能够归纳它的经验？也就是说，它能否学习到一些曾经没出现过的行为是好是坏。最后，我们还需要考虑行为体的延迟后果，也就是说，如果它获得了一个高回报，是因为它刚刚采取的行为，还是因为更早采取的行动。

2. 强化学习概述

2.1 序列决策

通常我们考虑制定一个好的决策序列的问题。为了标准化这个问题，在离散的设置中，一个行为体进行一个动作（actions）序列 ${a_{t}}$