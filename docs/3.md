# Lecture 3 Model Free Policy Evaluation: Policy Evaluation Without Knowing How the World Works

# 课时3 无模型策略评估 2018.03.20

## 4. 无模型策略评估

回忆上节内容，我们讨论了三个愈加复杂的问题：

1. 马尔可夫过程（MP）是一个具有马尔可夫性质的随机过程。

2. 马尔可夫奖励过程（MRP）是一个每个时间步都有奖励的马尔可夫过程，随时间衰减的奖励的累积成为价值（values）。

3. 马尔可夫决策过程（MDP）是一个在每个状态下都有选择或动作（actions）的马尔可夫奖励过程。

在上节课的下半部分，我们讨论了在 MDP 中评估一个策略的两种方法，分别是：直接解线性方程组和动态规划（dynamic programming）；我们也讨论了在 MDP 中寻找最优策略的三种方法，分别是：暴力策略搜索（brute force policy search）、策略迭代（policy iteration）和价值迭代（value iteration）。

这些方法都隐含这样一个假设，即每个状态转移的奖励和概率已知。然而，在许多情况下，这些信息并不容易被获取，因此我们需要无模型算法（model-free algorithms）。本课时我们讨论无模型策略评估（model-free policy evaluation），也就是说，在不知道奖励或状态转移概率的情况下，给定一个策略，我们对其价值进行评估。本课时我们暂时不讨论如何在无模型的情况下提升策略。

### 4.1 符号摘要

在深入讨论无模型策略评估方法之前，我们首先回顾一下本次课需要的有关 MDP 的一些符号。

我们将 MRP 的回报定义为随时间衰减的奖励的累积，起于时间步 $t$，止于 $H$，这里 $H$ 可能是无穷大，用数学的方式表示为
$$
G_t=\sum_{i=t}^{H-1}\gamma^{i-t}r_i,
\tag{1}
$$
这里 $0\leq t\leq H-1$，$\gamma$ 为衰减因子，$0<\gamma\leq 1$，$r_i$ 为在时间步 $i$ 的奖励。对于一个 MDP，回报 $G_t$ 被唯一定义，奖励 $r_i$ 根据策略 $\pi(a|s)$ 生成。

状态值函数 $V^{\pi}(s)$ 是起始于状态 $s$、遵照固定的策略 $\pi$ 得到的期望回报，我们可以将其表示为
$$
V^{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s]
\tag{2}
$$

$$
=\mathbb{E}_ {\pi}[\sum_{i=t}^{H-1}\gamma^{i-t}r_i|s_t=s]。
\tag{3}
$$

状态-行为值函数 $Q^{\pi}(s,a)$ 是起始于状态 $s$ 和动作 $a$、然后遵照固定的策略 $\pi$ 得到的期望回报，我们可以将其表示为
$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|s_t=s,a_t=a]
\tag{4}
$$

$$
=\mathbb{E}_ {\pi}[\sum_{i=t}^{H-1}\gamma^{i-t}r_i|s_t=s,a_t=a]。
\tag{5}
$$

在整个课程中，我们假设时间步有限，并且假设奖励、转移概率和策略是固定的，这些假设使得状态值函数和状态-行为值函数是独立于时间的，就像上节课推导的那样。

接下来我们介绍一个新的定义：历史（history）。

**定义 4.1** 历史（history）是行为体经历的状态、动作和奖励的排列。在情节域（episodic domains）中，我们也称之为片段（episode）。在考虑到许多交互时，我们用以下方式表示历史：第 $j$ 个历史为
$$
h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j}),
$$
这里 $L_j$ 是交互的长度，$s_{j,t}$、$a_{j,t}$、$r_{j,t}$分别是历史 $j$ 中的在时间步 $t$ 时的状态、动作和奖励。

### 4.2 动态规划（Dynamic Programming）

回忆上节课的用来计算有限 MDP 的值的动态规划算法。

# 算法1

以这种方式来写时，我们可以以多种方式来考虑 $V_k$。首先，$V_k(s)$ 是起始于状态 $s$，接下来 $k$ 次状态转移遵循策略 $\pi$ 的确切的值。其次，对于 $k$ 值很大或算法1终止的情况，$V_k(s)$ 是真实的无穷时间步情况下的 $V^{\pi}(s)$ 的估计。

我们可以通过图1来表示该算法。我们自上而下阅读这幅图，这里白色圆圈代表状态，黑色圆圈代表动作，弧线表示取期望值。这个图展示了起始于状态 $s$ 并转移两个时间步的分支效果，还说明了在状态 $s$ 依据策略 $\pi$ 采取一个动作后，我们如何去计算下一状态的值的期望。在动态规划中，我们使用当前的估计值 $V_{k-1}(s')$ 引导（bootstrap）或估计下一状态的值。

# 图1

### 4.3 蒙特卡洛在线策略评估（Monte Carlo On Policy Evaluation）

我们现在通过一个常见的计算方法来描述第一个无模型策略评估算法，这个方法被称为蒙特卡洛方法。我们首先抛开强化学习，详细叙述一个使用蒙特卡洛方法的例子，然后比较全面地讨论蒙特卡洛方法，最后在强化学习中应用蒙特卡洛方法。需要强调的是，这种方法仅在情节的环境中有效，我们将在本节中更仔细地研究算法时看到为什么会这样。

假设我们想估计今天从你家到斯坦福大学的通勤时间。假设我们有一个模拟器，这个模拟器可以模拟交通、天气、施工延误和其他变量的不确定性以及它们之间的相互作用。估计预期通勤时间的一种方法是在模拟器上模拟许多次，然后对这些仿真的通勤时间取平均值。这被称为我们的通勤时间的蒙特卡洛估计。

一般来说，我们在现实生活或仿真中通过观察多个迭代得到某个量的蒙特卡洛估计，然后将这些观测值取平均。根据大数定律，这个平均值会收敛到这个量的期望值。

在强化学习中，我们希望估计的量是 $V^{\pi}(s)$，也就是起始于状态 $s$、遵循策略 $\pi$ 的回报 $G_t$ 的平均值。因此，我们可以通过三步来得到 $V^{\pi}(s)$ 的蒙特卡洛估计：
1. 执行策略 $\pi$，直到终止。多次进行这个过程（rollout）。
2. 记录我们观察到的回报 $G_t$（起始于状态 $s$）。
3. 对我们获取的 $G_t$ 取平均值来估计 $V^{\pi}(s)$。

图2展示了蒙特卡洛策略评估，新的蓝色的线表示我们起始于状态 $s$，对整个片段进行采样直到终止。

# 图2

蒙特卡洛在线策略评估有两种方式，取决于每个 rollout 中，我们是只在第一次遇到一个状态时取平均值，还是每次遇到一个状态时都取平均值。这两种方式分别被称为首次访问（First-Visit）蒙特卡洛策略评估和每次访问（Every-Visit）蒙特卡洛策略评估。

更正式地，我们用算法2描述首次访问蒙特卡洛，用算法3描述每次访问蒙特卡洛。

# 算法2/算法3

注意，在算法2和3的 for 循环中，我们可以移去向量 $S$，并且以
$$
V^{\pi}(s_{j,t})\leftarrow V^{\pi}(s_{j,t})+\frac{1}{N(s_{j,t})}(G_{j,t}-V^{\pi}(s_{j,t}))
\tag{6}
$$
代替 $V^{\pi}(s_{j,t})$ 的更新。这是因为新的平均值可以用以下方式来计算：
$$
\frac{V^{\pi}(s_{j,t})\times (N(s_{j,t})-1)+G_{j,t}}{N(s_{j,t})}=V^{\pi}(s_{j,t})+\frac{1}{N(s_{j,t})}(G_{j,t}-V^{\pi}(s_{j,t}))。
\tag{7}
$$
用 $\alpha$ 替代 $\frac{1}{N(s_{j,t})}$，我们就得到了更一般的增量蒙特卡洛策略评估（Incremental Monte Carlo On Policy Evaluation）。算法4和算法5分别详细描述了首次访问和每次访问蒙特卡洛。

# 算法4/算法5

若我们使 $\alpha=\frac{1}{N(s_{j,t})}$，便得到了算法2和算法3中展示的原始的蒙特卡洛策略评估；若$\alpha>\frac{1}{N(s_{j,t})}$，则表示我们为新数据赋予更大的权重，这有助于学习非线性空间。如果我们是在确切的马尔可夫空间，每次访问蒙特卡洛将会是更加数据高效的，因为每次我们访问一个状态都会更新我们的回报平均值。

**练习 4.1** 回忆上节课提到的火星探测器（Mars Rover）MDP，如图三所示。假设我们当前对各个状态的估计值为 $0$，如果我们经历历史
$$
h=(S3,TL,+0,S2,TL,+0,S1,TL,+1,terminal)，
$$
那么：
1. 每个状态的 $V$ 的首次访问蒙特卡洛估计是什么？
2. 每个状态的 $V$ 的每次访问蒙特卡洛估计是什么？
3. 当 $\alpha=\frac{2}{3}$ 时，$V$ 的增量首次访问蒙特卡洛估计是什么？
4. 当 $\alpha=\frac{2}{3}$ 时，$V$ 的增量每次访问蒙特卡洛估计是什么？

# 图3

### 4.4 蒙特卡洛离线策略评估（Monte Carlo Off Policy Evaluation）

上一节我们讨论了在希望评估的策略 $\pi$ 下，我们能够获得许多 $G_t$ 的实现的情况。然而在许多昂贵或高风险的情况下，我们并不能以期望评估的策略去获取 $G_t$ 的rollout，例如，我们可能有与某一项医疗政策相关的数据，但我们想要确定另一个不同的医疗策略的价值。在本节中，我们讨论蒙特卡洛离线策略评估（Monte Carlo off policy evaluation），这是一种用从一个策略中获得的数据去评估另一个不同的策略的方法。

#### 4.4.1 重要性采样（Importance Sampling）

重要性测样（importance sampling）方法是离线策略评估的关键因素。重要性采样的目标是仅使用 $f(x_1), ..., f(x_n)$ 估计函数 $f(x)$ 的期望值，这里 $x$ 源于分布 $q$， $x_i$ 源于另一个不同的分布 $p$。总而言之，对于 $1\leq x_i \leq n$，给定 $q(x_i),p(x_i),f(x_i)$，我们想要估计 $\mathbb{E}_{x~q}[f(x)]$。我们可以通过以下近似来实现这个估计：
$$
\mathbb{E}_{x~q}[f(x)]=\int_x {q(x)f(x)} dx
\tag{8}
$$

$$
=\int_x {p(x)[\frac{q(x)}{p(x)}f(x)]}dx
\tag{9}
$$