# Lecture 3 Model Free Policy Evaluation: Policy Evaluation Without Knowing How the World Works

# 课时3 无模型策略评估 2018.03.20

## 4. 无模型策略评估

回忆上节内容，我们讨论了三个愈加复杂的问题：

1. 马尔可夫过程（MP）是一个具有马尔可夫性质的随机过程。

2. 马尔可夫奖励过程（MRP）是一个每个时间步都有奖励的马尔可夫过程，随时间衰减的奖励的累积成为价值（values）。

3. 马尔可夫决策过程（MDP）是一个在每个状态下都有选择或动作（actions）的马尔可夫奖励过程。

在上节课的下半部分，我们讨论了在 MDP 中评估一个策略的两种方法，分别是：直接解线性方程组和动态规划（dynamic programming）；我们也讨论了在 MDP 中寻找最优策略的三种方法，分别是：暴力策略搜索（brute force policy search）、策略迭代（policy iteration）和价值迭代（value iteration）。

这些方法都隐含这样一个假设，即每个状态转移的奖励和概率已知。然而，在许多情况下，这些信息并不容易被获取，因此我们需要无模型算法（model-free algorithms）。本课时我们讨论无模型策略评估（model-free policy evaluation），也就是说，在不知道奖励或状态转移概率的情况下，给定一个策略，我们对其价值进行评估。本课时我们暂时不讨论如何在无模型的情况下提升策略。

### 4.1 符号摘要

在深入讨论无模型策略评估方法之前，我们首先回顾一下本次课需要的有关 MDP 的一些符号。

我们将 MRP 的回报定义为随时间衰减的奖励的累积，起于时间 t，止于 H，这里 H 可能是无穷大，用数学的方式表示为：
$$
G_t=\sum_{i=t}^{H-1}\gamma^{i-t}t_i,
\tag{1}
$$
这里 $0\leqt\leqH-1$，