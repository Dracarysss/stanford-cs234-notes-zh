# Lecture 3 Model Free Policy Evaluation: Policy Evaluation Without Knowing How the World Works

# 课时3 无模型策略评估 2018.03.20

## 4. 无模型策略评估

回忆上节内容，我们讨论了三个愈加复杂的问题：

1. 马尔可夫过程（MP）是一个具有马尔可夫性质的随机过程。

2. 马尔可夫奖励过程（MRP）是一个每个时间步都有奖励的马尔可夫过程，随时间衰减的奖励的累积成为价值（values）。

3. 马尔可夫决策过程（MDP）是一个在每个状态下都有选择或动作（actions）的马尔可夫奖励过程。

在上节课的下半部分，我们讨论了在 MDP 中评估一个策略的两种方法，分别是：直接解线性方程组和动态规划（dynamic programming）；我们也讨论了在 MDP 中寻找最优策略的三种方法，分别是：暴力策略搜索（brute force policy search）、策略迭代（policy iteration）和价值迭代（value iteration）。

这些方法都隐含这样一个假设，即每个状态转移的奖励和概率已知。然而，在许多情况下，这些信息并不容易被获取，因此我们需要无模型算法（model-free algorithms）。本课时我们讨论无模型策略评估（model-free policy evaluation），也就是说，在不知道奖励或状态转移概率的情况下，给定一个策略，我们对其价值进行评估。本课时我们暂时不讨论如何在无模型的情况下提升策略。

### 4.1 符号摘要

在深入讨论无模型策略评估方法之前，我们首先回顾一下本次课需要的有关 MDP 的一些符号。

我们将 MRP 的回报定义为随时间衰减的奖励的累积，起于时间步 $t$，止于 $H$，这里 $H$ 可能是无穷大，用数学的方式表示为
$$
G_t=\sum_{i=t}^{H-1}\gamma^{i-t}r_i,
\tag{1}
$$
这里 $0\leq t\leq H-1$，$\gamma$ 为衰减因子，$0<\gamma\leq 1$，$r_i$ 为在时间步 $i$ 的奖励。对于一个 MDP，回报 $G_t$ 被唯一定义，奖励 $r_i$ 根据策略 $\pi(a|s)$ 生成。

状态值函数 $V^{\pi}(s)$ 是起始于状态 $s$、遵照固定的策略 $\pi$ 得到的期望回报，我们可以将其表示为
$$
V^{\pi}(s)=\mathbb{E}_{\pi}[G_t|s_t=s]
\tag{2}
=\mathbb{E}_{\pi}[\sum_{i=t}^{H-1}\gamma^{i-t}r_i|s_t=s]。
\tag{3}
$$

状态-行为值函数 $Q^{\pi}(s,a)$ 是起始于状态 $s$ 和动作 $a$、然后遵照固定的策略 $\pi$ 得到的期望回报，我们可以将其表示为
$$
Q^{\pi}(s,a)=\mathbb{E}_{\pi}[G_t|s_t=s,a_t=a]
\tag{4}
=\mathbb{E}_{\pi}[\sum_{i=t}^{H-1}\gamma^{i-t}r_i|s_t=s,a_t=a]。
\tag{5}
$$

在整个课程中，我们假设时间步有限，并且假设奖励、转移概率和策略是固定的，这些假设使得状态值函数和状态-行为值函数是独立于时间的，就像上节课推导的那样。

接下来我们介绍一个新的定义：历史（history）。

**定义 4.1** 历史（history）是行为体经历的状态、动作和奖励的排列，通常我们也称之为片段（episode）。在考虑到许多交互时，我们用以下方式表示历史：第 $j$ 个历史为
$$
h_j=(s_{j,1},a_{j,1},r_{j,1},s_{j,2},a_{j,2},r_{j,2},...,s_{j,L_j}),
$$
这里 $L_j$ 是交互的长度，$s_{j,t}$、$a_{j,t}$、$r_{j,t}$分别是历史 $j$ 中的在时间步 $t$ 时的状态、动作和奖励。

### 4.2 动态规划

回忆上节课的用来计算有限 MDP 的值的动态规划算法。
___
**算法1** 对于固定策略 $\pi$，计算 MDP 值函数的迭代算法
___
1：