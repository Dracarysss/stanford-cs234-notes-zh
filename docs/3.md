# Lecture 3 Model Free Policy Evaluation: Policy Evaluation Without Knowing How the World Works

# 课时3 无模型策略评估 2018.03.20

## 4. 无模型策略评估

回忆上节内容，我们讨论了三个愈加复杂的问题：

1. 马尔可夫过程（MP）是一个具有马尔可夫性质的随机过程。

2. 马尔可夫奖励过程（MRP）是一个每个时间步都有奖励的马尔可夫过程，随时间衰减的奖励的累积成为价值（values）。

3. 马尔可夫决策过程（MDP）是一个在每个状态下都有选择或动作（actions）的马尔可夫奖励过程。

在上节课的下半部分，我们讨论了在 MDP 中评估一个策略的两种方法，分别是：直接解线性方程组和动态规划（dynamic programming）；我们也讨论了在 MDP 中寻找最优策略的三种方法，分别是：暴力策略搜索（brute force policy search）、策略迭代（policy iteration）和价值迭代（value iteration）。

这些方法都隐含这样一个假设，即每个状态转移的奖励和概率已知。然而，在许多情况下，这些信息并不容易被获取，因此我们需要无模型算法（model-free algorithms）。本课时我们讨论无模型策略评估（model-free policy evaluation），也就是说，在不知道奖励或状态转移概率的情况下，给定一个策略，我们对其价值进行评估。本课时我们暂时不讨论如何在无模型的情况下提升策略。

### 4.1 符号摘要

在深入讨论无模型策略评估方法之前，我们首先回顾一下本次课需要的有关 MDP 的一些符号。

我们将 MRP 的回报定义为随时间衰减的奖励的累积，起于时间步 t，止于 H，这里 H 可能是无穷大，用数学的方式表示为：
$$
G_t=\sum_{i=t}^{H-1}\gamma^{i-t}r_i,
\tag{1}
$$
这里 $0\leq t\leq H-1$，$\gamma$ 为衰减因子，$0<\gamma\leq 1$，$r_i$ 为在时间步 $i$ 的奖励。对于一个 MDP，回报 $G_t$ 被唯一定义，奖励 $r_i$ 根据策略 $\pi(a|s)$ 生成。状态值函数 $V^{\pi}(s)$ 是根据固定的策略 $\pi$ 起始于状态 s 的期望回报，我们可以将其表示为：
$$
V^{\pi}(s)&=\mathbb{E}_{\pi}[G_t|s_t=s]
\tag{2}
&=\mathbb{E}_{\pi}[\sum_{i=t}^{H-1}\gamma^{i-t}r_i|s_t=s]。
\tag{3}
$$