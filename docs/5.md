# Lecture 5 Value Function Approximation

# 课时4 值函数近似 2018.03.20

## 7. 介绍（Introduction）

到目前为止我们介绍了基于查表（lookup table）的值函数，这里每个状态都有一个对应条目，$V(s)$，或每个状态-动作对都有一个对应的条目，$Q(s,a)$。然而，这种方法可能不适合处理动作空间或状态空间非常大、或那些我们希望快速学习每个状态估计值的情况。函数近似是解决这个问题的常用办法：
$$
v_{\pi}(s) \approx \hat{v}(s,\mathbf{w}) \text{ or } q_{\pi}(s,a) \approx \hat{q}(s,a,\mathbf{w})，
$$
这里 $\mathbf{w}$ 通常指近似函数的参数。以下是常见的近似函数：

$\bullet$ 特征的线性组合（Linear combinations of features）

$\bullet$ 神经网络（Neural networks）

$\bullet$ 决策树（Decision trees）

$\bullet$ 最近邻（Nearest neighbors）

$\bullet$ 傅立叶/小波基（Fourier / wavelet bases）

我们讲进一步讨论可导近似函数：线性特征表示和神经网络，接下来的部分会提到要求函数可导的原因。

## 8. 线性特征表示（Linear Feature Representations）

在线性特征表示中，我们用一个特征向量去表示一个状态：
$$
x(s)=[x_1(s) x_2(s) ... x_n(s)]，
$$
然后我们用特征的线性组合来近似值函数：
$$
\hat{v}(s,\mathbf{w})=x(s)^{\text{T}}\mathbf{w}=\sum_{j=1}^n x_j(s)\mathbf{w}_j。
$$
我们定义（二次的）目标函数（也被称为损失函数）为：
$$
J(\mathbf{w})=\mathbb{E}_ {\pi}[(v_{\pi}(s)-\hat{v}(s,\mathbf{w}))^2]