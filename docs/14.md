# Lecture 14 Model Based RL, Monte-Carlo Tree Search

# 课时14 基于模型的强化学习，蒙特卡洛树搜索 2019.03.06

## 1. 介绍（Introduction）

本节课我们讨论基于模型的强化学习与基于仿真的树搜索方法。到目前为止我们已经讨论了从经历中尝试学习值函数或策略的方法，与这些方法相反，基于模型的方法首先从经历中学习环境的模型，然后使用这个模型做出规划和行动。基于模型的方法在某些情况下有更高的采样效率和更快的收敛速度。我们还将讨论 MCTS 及其变体，它们可以用于针对给出的模型做出规划。MCTS 是 AlphoGo 成功背后的主要思想之一。

**图一**

## 2. 模型学习（Model Learning）

我们用 $<S,A,R,T,\gamma>$ 来表示一个 MDP 的模型，由 $\mu$ 参数化。在模型学习中，我们假设状态空间 $S$ 和 动作空间 $A$ 已知，并且通常还假设状态转移与奖励是相互独立的，即

$$
P[s_{t+1},r_{t+1}|s_t,a_t] = P[s_{t+1}|s_t,a_t]P[r_{t+1}|s_t,a_t]。
$$

因此，模型学习包括两个部分，分别为奖励函数 $R(\cdot|s,a)$ 和转移分布 $P(\cdot|s,a)$。

给定一个真实轨迹的集合 ${S_t^k,A_t^k,R_t^k,...,S_T^k}_{k=1}^{K}$，模型学习可以被视为一个监督学习问题，学习奖励函数 $R(s,a)$ 是一个回归问题，而学习转移函数 $P(s'|s,a)$ 是一个密度估计问题。首先我们选取一类合适的参数化模型，如查表模型、线性期望、线性高斯、高斯过程、深度神经网络等，然后我们选择一个恰当的损失函数，如均方误差、KL 散度等，通过最小化这个损失来优化参数。

## 3. 规划（Planning）

给定一个学习到的环境的模型，规划可以由基于值的方法、策略搜索的方法或树搜索的方法来实现。

一种比较的规划方法的思路为：仅使用该模型来生成采样轨迹，并使用 Q-学习、蒙特卡洛控制或 SARSA 等方法进行控制。这种基于样本的规划方法通常更具数据效率。