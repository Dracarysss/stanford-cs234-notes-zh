# Lecture 4 Model Free Control

# 课时4 无模型控制 2018.03.20

## 5. 无模型控制（Model Free Control）

上次课我们讨论了如何在不知道世界的模型、仅与环境交互的情况下评估一个给定的策略，我们提出了几种无模型策略评估方法：蒙特卡洛策略评估和 TD 学习。这节课我们讨论无模型控制（model-free control），这里我们要在相同的约束下（只有交互，不知道状态转移概率或奖励）学习好的策略。这个框架在以下两种情况中是很重要的：
1. MDP 模型未知，但我们可以从这个 MDP 中对轨迹采样，或
2. MDP 模型已知，但由于计算量的原因，我们无法通过有模型控制方法计算价值函数。

本节课我们仍基于表格的设定，也就是说我们可以将每个状态值或状态-行为值作为表格的元素。下节课我们将基于值函数近似的设定，重新检查今天和之前的课中提及的算法，这包括尝试将一个函数转换为状态值函数或状态-行为值函数。

### 5.1 广义策略迭代（Generalized Policy Iteration）

首先回忆有模型策略迭代算法，如算法1所示。

# 算法1

上节课我们介绍了无模型策略迭代算法，所以我们可以用无模型的方式将第四行替换。然而，为了使整个算法是无模型的，我们必须找到一种方式去处理第五行。根据定义，我们有 $Q^{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)V^{\pi}(s^{'})$，因此，我们可以通过在模型策略迭代算法中使用这个值，从而得到无模型策略迭代算法（算法2）。

# 算法2

由于我们在第五行做了替换，有一些事项需要注意：
1. 如果策略 $\pi$ 是确定的，或不是以正的概率生成每一个动作 $a$，那么我们无法确切地计算第五行中的最大值。
2. 通过策略迭代算法，我们得到的是 $Q^{\pi}$ 的估计，所以我们并不清楚第五行是否会像有模型情况那样单调地提升策略。

### 5.2 探索的重要性（Importance of Exploration）

#### 5.2.1 探索（Importance）

上一部分里，我们看到无模型策略迭代算法的一个注意事项是策略 $\pi$ 需要对每个动作赋予一个正的概率，这样每个状态-行为值才能被确定。换句话说，策略 $\pi$ 应该探索动作，即使这些动作根据当前的 $Q$ 值估计来看可能不是最优的。

#### 5.2.2 $\epsilon$-贪婪策略（$\epsilon$-greedy Policies）

为了探索那些根据当前的 $Q$ 值估计来看可能不是最优的动作，我们需要需要一个系统的方式去平衡对非最优动作的探索与对最优（贪婪）动作的利用。一个简单的方法是以一个小的概率去采取一个随机动作，其余时间采用贪婪动作，这种探索方式被称为 $\epsilon$-贪婪策略（$\epsilon$-greedy policy），基于状态-行为值 $Q^{\pi}(s,a)$，我们可以用如下的数学方式来表示 $\epsilon$-贪婪策略：
$$
\pi(a|s)=
\begin{cases}
a, & \text{with probability $\frac{\epsilon}{|A|}$} \\
\mathop{\arg\max}_{a} Q^{\pi}(s,a), & \text{with probability $1-\epsilon$}
\end{cases}
$$

#### 5.2.3 单调 $\epsilon$-贪婪策略提升（Monotonic $\epsilon$-greedy Policy Improvement）

第二节课中通过策略提升定理我们看到，如果基于当前的值采用贪婪动作，然后一直根据策略 $\pi$ 选取动作，那么这个策略相对策略 $\pi$ 来说是一个提升。那么，基于策略 $\pi$ 的 $\epsilon$-贪婪策略是否也是策略 $\pi$ 的一个提升呢？这个问题有助于解决算法2的第二个注意事项。幸运的是，关于 $\epsilon$-贪婪策略，有一个与策略提升定理类似的定理。下面我们介绍并推导这个定理。

**定理 5.1** （单调 $\epsilon$-贪婪策略提升，Monotonic $\epsilon$-greedy Policy Improvement）令 $\pi_i$ 为一个 $\epsilon$-贪婪策略，那么，基于 $Q^{\pi_i}$ 的 $\epsilon$-贪婪策略，记为 $\pi_{i+1}$，是相对策略 $\pi_i$ 单调提升的，也就是说，$V^{\pi_{i+1}}\geq V^{\pi_{i}}$。

证明：我们首先证明对于所有状态 $s$，有 $Q^{\pi_i}(s,\pi_{i+1}(s)) \geq V^{\pi_{i}}(s)$。
$$
Q^{\pi_i}(s,\pi_{i+1}(s)) = \sum_{a\in A}\pi_{i+1}(a|s)Q^{\pi_i}(s,a)
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'})
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'}) \frac{1-\epsilon}{1-\epsilon}
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'}) \sum_{a\in A}\frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon}
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\sum_{a\in A} \frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon} \mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'})
$$

$$
\geq \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\sum_{a\in A} \frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon} Q^{\pi_i}(s,a)
$$

$$
= \sum_{a\in A}\pi_{i}(a|s)Q^{\pi_i}(s,a)
$$

$$
= V^{\pi_i}(s)
$$
第一个等式是因为我们根据策略 $\pi_{i+1}$ 生成第一个动作，然后根据策略 $\pi_{i}$ 生成之后的动作。第四个等式是因为 $1-\epsilon = \sum_{a}[\pi_{i}(a|s)-\frac{\epsilon}{|A|}]$。

根据策略提升定理，我们知道 $Q^{\pi_i}(s,\pi_{i+1}(s)) \geq V^{\pi_{i}}(s)$ 意味着对于所有状态 $s$，$V^{\pi_{i+1}}(s) \geq V^{\pi_{i}}(s)$。证明完毕。$\diamondsuit$