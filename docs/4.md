# Lecture 4 Model Free Control

# 课时4 无模型控制 2018.03.20

## 5. 无模型控制（Model Free Control）

上次课我们讨论了如何在不知道世界的模型、仅与环境交互的情况下评估一个给定的策略，我们提出了几种无模型策略评估方法：蒙特卡洛策略评估和 TD 学习。这节课我们讨论无模型控制（model-free control），这里我们要在相同的约束下（只有交互，不知道状态转移概率或奖励）学习好的策略。这个框架在以下两种情况中是很重要的：
1. MDP 模型未知，但我们可以从这个 MDP 中对轨迹采样，或
2. MDP 模型已知，但由于计算量的原因，我们无法通过有模型控制方法计算价值函数。

本节课我们仍基于表格的设定，也就是说我们可以将每个状态值或状态-行为值作为表格的元素。下节课我们将基于值函数近似的设定，重新检查今天和之前的课中提及的算法，这包括尝试将一个函数转换为状态值函数或状态-行为值函数。

### 5.1 广义策略迭代（Generalized Policy Iteration）

首先回忆有模型策略迭代算法，如算法1所示。

# 算法1

上节课我们介绍了无模型策略迭代算法，所以我们可以用无模型的方式将第四行替换。然而，为了使整个算法是无模型的，我们必须找到一种方式去处理第五行。根据定义，我们有 $Q^{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)V^{\pi}(s^{'})$，因此，我们可以通过在模型策略迭代算法中使用这个值，从而得到无模型策略迭代算法（算法2）。

# 算法2

由于我们在第五行做了替换，有一些事项需要注意：
1. 如果策略 $\pi$ 是确定的，或不是以正的概率生成每一个动作 $a$，那么我们无法确切地计算第五行中的最大值。
2. 通过策略迭代算法，我们得到的是 $Q^{\pi}$ 的估计，所以我们并不清楚第五行是否会像有模型情况那样单调地提升策略。

### 5.2 探索的重要性（Importance of Exploration）

#### 5.2.1 探索（Importance）

上一部分里，我们看到无模型策略迭代算法的一个注意事项是策略 $\pi$ 需要对每个动作赋予一个正的概率，这样每个状态-行为值才能被确定。换句话说，策略 $\pi$ 应该探索动作，即使这些动作根据当前的 $Q$ 值估计来看可能不是最优的。

#### 5.2.2 $\epsilon$-贪婪策略（$\epsilon$-greedy Policies）

为了探索那些根据当前的 $Q$ 值估计来看可能不是最优的动作，我们需要需要一个系统的方式去平衡对非最优动作的探索与对最优（贪婪）动作的利用。一个简单的方法是以一个小的概率去采取一个随机动作，其余时间采用贪婪动作，这种探索方式被称为 $\epsilon$-贪婪策略（$\epsilon$-greedy policy），基于状态-行为值 $Q^{\pi}(s,a)$，我们可以用如下的数学方式来表示 $\epsilon$-贪婪策略：
$$
\pi(a|s)=
\begin{cases}
a, & \text{with probability $\frac{\epsilon}{|A|}$} \\
\mathop{\arg\max}_{a} Q^{\pi}(s,a), & \text{with probability $1-\epsilon$}
\end{cases}
$$

#### 5.2.3 单调 $\epsilon$-贪婪策略提升（Monotonic $\epsilon$-greedy Policy Improvement）

第二节课中通过策略提升定理我们看到，如果基于当前的值采用贪婪动作，然后一直根据策略 $\pi$ 选取动作，那么这个策略相对策略 $\pi$ 来说是一个提升。那么，基于策略 $\pi$ 的 $\epsilon$-贪婪策略是否也是策略 $\pi$ 的一个提升呢？这个问题有助于解决算法2的第二个注意事项。幸运的是，关于 $\epsilon$-贪婪策略，有一个与策略提升定理类似的定理。下面我们介绍并推导这个定理。

**定理 5.1** （单调 $\epsilon$-贪婪策略提升，Monotonic $\epsilon$-greedy Policy Improvement）令 $\pi_i$ 为一个 $\epsilon$-贪婪策略，那么，基于 $Q^{\pi_i}$ 的 $\epsilon$-贪婪策略，记为 $\pi_{i+1}$，是相对策略 $\pi_i$ 单调提升的，也就是说，$V^{\pi_{i+1}}\geq V^{\pi_{i}}$。

证明：我们首先证明对于所有状态 $s$，有 $Q^{\pi_i}(s,\pi_{i+1}(s)) \geq V^{\pi_{i}}(s)$。
$$
Q^{\pi_i}(s,\pi_{i+1}(s)) = \sum_{a\in A}\pi_{i+1}(a|s)Q^{\pi_i}(s,a)
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'})
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'}) \frac{1-\epsilon}{1-\epsilon}
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'}) \sum_{a\in A}\frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon}
$$

$$
= \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\sum_{a\in A} \frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon} \mathop{\max}_{a^{'}}Q^{\pi_i}(s,a^{'})
$$

$$
\geq \frac{\epsilon}{|A|} \sum_{a\in A}Q^{\pi_i}(s,a) + (1-\epsilon)\sum_{a\in A} \frac{\pi_{i}(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon} Q^{\pi_i}(s,a)
$$

$$
= \sum_{a\in A}\pi_{i}(a|s)Q^{\pi_i}(s,a)
$$

$$
= V^{\pi_i}(s)
$$
第一个等式是因为我们根据策略 $\pi_{i+1}$ 生成第一个动作，然后根据策略 $\pi_{i}$ 生成之后的动作。第四个等式是因为 $1-\epsilon = \sum_{a}[\pi_{i}(a|s)-\frac{\epsilon}{|A|}]$。

根据策略提升定理，我们知道 $Q^{\pi_i}(s,\pi_{i+1}(s)) \geq V^{\pi_{i}}(s)$ 意味着对于所有状态 $s$，$V^{\pi_{i+1}}(s) \geq V^{\pi_{i}}(s)$。证明完毕。$\diamondsuit$

因此，如果我们基于当前的 $\epsilon$-贪婪策略进行 $\epsilon$-贪婪动作，我们的策略实际上是提升的。

#### 5.2.4 无限探索中的极限贪心策略（Greedy in the Limit of Infinite Exploration）

上面我们介绍了一种 $\epsilon$-贪婪策略的简单方式以平衡对新动作的探索和对已有知识的利用。我们还可以通过引入一种新的、保证算法收敛的探索策略来改善这种平衡，这种策略就是无限探索中的极限贪心策略（Greedy in the Limit of Infinite Exploration, GLIE）。

**定义 5.1** （Greedy in the Limit of Infinite Exploration, GLIE）如果策略 $\pi$ 满足以下两条性质，那么它是无限探索中的极限贪心策略：
1. 所有的状态-行为对被访问无限多次，即对于所有的 $s\in S$，$a\in A$，有
$$
\lim_{i\to\infty} N_i(s,a) \rightarrow \infty，
$$
这里 $N_i(s,a)$ 为直到（包括）片段 $i$ 在状态 $s$ 采用动作 $a$ 的次数。
2. 行为策略收敛到基于学习到的 $Q$ 函数的贪婪策略，即对于所有的 $s\in S$，$a\in A$，有
$$
\lim_{i\to\infty} \pi_i(a|s) = \mathop{\arg\max}_{a} q(s,a) \text{ with probability $1$}。
$$
GLIE 策略的一个例子是 $\epsilon_i=\frac{1}{i}$ 的 $\epsilon$-贪婪策略，这里 $i$ 为片段的序号，$\epsilon$ 逐渐减小到 $0$。可以看出，对所有的 $i$，$\epsilon_i>0$，在每个时间步我们都以一个正的概率进行探索，因此满足 GLIE 的第一个条件；由于当 $i \rightarrow \infty$，$\epsilon_i \rightarrow 0$，极限情况下这个策略是贪婪的，因此满足 GLIE 的第二个条件。

### 5.3 蒙特卡洛控制

现在我们将上述探索策略和上节课讨论的无模型策略评估算法结合起来，以得到一些无模型控制方法。算法3展示了首次访问在线蒙特卡洛控制；如果不检查第七行中的首次访问，我们就得到了每次访问在线蒙特卡洛控制。

# 算法3

像前面说过的，GLIE 策略保证我们的无模型控制方法收敛，特别低，我们有以下结果：

**定理 5.2** GLIE 蒙特卡洛控制收敛到最优状态-行为值函数，即 $Q(s,a)\rightarrow q(s,a)$。

### 5.4 时间差分控制

上节课我们还介绍了另一种无模型策略评估方法：TD(0)。现在我们将上述的探索策略与 $TD(0)$ 结合起来：在线策略（on-policy）和离线策略(off-policy)。我们首先介绍在线策略的方法，这种方法也被称为 SARSA，如算法4所示。

# 算法4

算法4的第十行为策略评估更新，第十一行为策略提升。SARSA的名字源于用于更新方程的轨迹，为了更新状态-行为对 $(s,a)$ 的 $Q$ 值，我们需要奖励、下一状态以及下一动作，因此我们需要 $(s,a,r,s^{'},a^{'})$。SARSA 是在线策略的方法，因为用于更新方程的动作 $a$ 和$a^{'}$ 都源于更新时存在的（最新的）策略。

像蒙特卡洛那样，我们可以得到 SARSA 的收敛性，但我们需要一个额外的条件：

**定理 5.3** 对于有限状态和有限动作的 MDP，如果满足以下两个条件，那么 SARSA 会收敛到最优状态-行为值，即 $Q(s,a)\rightarrow q(s,a)$：
1. 策略 $\pi$ 的序列是 GLIE。
2. 步长 $a_t$ 满足罗宾斯-蒙罗（Robbins-Munro）序列使得：
$$
\sum_{t=1}^{\infty} \alpha_t=\infty，
$$

$$
\sum_{t=1}^{\infty} \alpha_t^2<\infty。
$$