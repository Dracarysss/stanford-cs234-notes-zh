# Lecture 4 Model Free Control

# 课时4 无模型控制 2018.03.20

## 5. 无模型控制

上次课我们讨论了如何在不知道世界的模型、仅与环境交互的情况下评估一个给定的策略，我们提出了几种无模型策略评估方法：蒙特卡洛策略评估和 TD 学习。这节课我们讨论无模型控制（model-free control），这里我们要在相同的约束下（只有交互，不知道状态转移概率或奖励）学习好的策略。这个框架在以下两种情况中是很重要的：
1. MDP 模型未知，但我们可以从这个 MDP 中对轨迹采样，或
2. MDP 模型已知，但由于计算量的原因，我们无法通过有模型控制方法计算价值函数。

本节课我们仍基于表格的设定，也就是说我们可以将每个状态值或状态-行为值作为表格的元素。下节课我们将基于值函数近似的设定，重新检查今天和之前的课中提及的算法，这包括尝试将一个函数转换为状态值函数或状态-行为值函数。

### 5.1 广义策略迭代

首先回忆有模型策略迭代算法，如算法1所示。

# 算法1

上节课我们介绍了无模型策略迭代算法，所以我们可以用无模型的方式将第四行替换。然而，为了使整个算法是无模型的，我们必须找到一种方式去处理第五行。根据定义，我们有 $Q^{\pi}(s,a)=R(s,a)+\gamma \sum_{s^{'}\in S}P(s^{'}|s,a)V^{\pi}(s^{'})$，