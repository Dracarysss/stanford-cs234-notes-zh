# Lecture 6 CNNs and Deep Q-learning

# 课时6 卷积神经网络与深度 Q-学习 2018.03.20

## 7. 基于值的深度强化学习（Value-based Deep Reinforcement Learning）

这一部分我们介绍三种常见的基于值的深度强化学习（RL）算法：深度 Q-网络（Deep Q-network）[1]，双 DQN（Double DQN）[2] 和对抗 DQN（Dueling DQN）[3]。通过端到端的（end-to-end）强化学习，这三种神经网络架构都可以直接从像电子游戏的预处理像素这样的高维输入（high-dimensional inputs）中学习到成功的策略，它们在 Atari 2600 的49个游戏中达到了相当于专业的游戏测试员的水平 [4]。

这些架构都使用了卷积神经网络（Convolutional Neural Networks, CNNs）[5] 来从像素输入中提取特征。理解 CNNs 特征提取的机制有助于我们理解 DQN 的工作方式。 斯坦福大学 CS231N 课程网站提供了关于 CNNs 的详尽的介绍和例子，通过这个链接，读者可以阅读更多细节：http://cs231n.github.io/convolutional-networks/。本节的其余部分我们将关注 RL 和 基于值的深度 RL 算法的泛化。

### 7.1 概述：状态-行为值函数近似（Recap: Action-value Function Approximation）

上节课我们用参数化的近似函数来表示状态-行为值函数（Q-函数），若我们将参数记为 $\mathbf{w}$，那么这种设定中，Q-函数可以被表示为 $\hat{q}(s,a,\mathbf{w})$。

假设我们知道 $q(s,a)$，通过最小化真实状态-动作值函数 $q(s,a)$ 和近似估计之间的均方误差 $J(\mathbf{w})$，我们可以得到近似的 Q-函数：
$$
J(\mathbf{w})=\mathbb{E}[(q(s,a)-\hat{q}(s,a,\mathbf{w}))^2]。
\tag{1}
$$

我们可以通过随机梯度下降（stochastic gradient descent, SGD）找到 $J$ 的关于 $\mathbf{w}$ 的局部最小值，并且以下式来更新 $\mathbf{w}$：
$$
\Delta(\mathbf{w})=-\frac{1}{2}\alpha\nabla_{\mathbf{w}}J(\mathbf{w})=\alpha\mathbb{E}[(q(s,a)-\hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})]，
\tag{2}
$$
这里 $\alpha$ 为学习率（learning rate）。通常情况下，真实的状态-动作值函数 $q(s,a)$ 是未知的，所以用一个近似的学习目标（learning target）来代替式（2）中的 $q(s,a)$。

在蒙特卡洛方法中，对于片段式的 MDPs，我们用无偏的回报 $G_t$ 来代替学习目标：
$$
\Delta(\mathbf{w})=\alpha(G_t-\hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})。
\tag{3}
$$

对于 SARSA 方法，我们使用基于推导的 TD 目标 $r+\gamma\hat{q}(s',a',\mathbf{w})$：
$$
\Delta(\mathbf{w})=\alpha(r+\gamma\hat{q}(s',a',\mathbf{w})-\hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})，
\tag{4}
$$
这里 $a'$ 为在下一状态 $s'$ 执行的动作，$\gamma$ 为衰减因子，TD 目标利用了当前的函数近似值。

对于 Q-学习，我们使用 TD 目标 $r+\gamma\mathop{\max}_{a'}\hat{q}(s',a',\mathbf{w})$，并以下式来更新 $\mathbf{w}$：

$$
\Delta(\mathbf{w})=\alpha(r+\gamma\mathop{\max}_ {a'}\hat{q}(s',a',\mathbf{w})-\hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})。
\tag{5}
$$

下面的部分我们将介绍如何使用深度神经网络来近似 $\hat{q}(s,a,\mathbf{w})$，以及如何通过端到端的训练来学习神经网络的参数 $\mathbf{w}$。

### 7.2 泛化：深度 Q-网络（Generalization: Deep Q-network）[1]

线性近似函数的表现非常依赖于特征的质量，一般来说，手动给出合适的特征很困难，也很耗时。为了在大的空间（large domians）（比如大的状态空间）进行决策并实现特征自动提取，深度神经网络（DNNs）通常被选作近似函数。

#### 7.2.1 DQN 架构（DQN Architecture）

图1展示了 DQN 的结构，图中的网络将 Atari 游戏环境的预处理像素图（预处理见 7.2.2 节）作为输入，为每个可行的动作赋予一个 Q 值，将这些 Q 值作为一个向量输出。预处理的像素输入代表了游戏状态 $s$，单个的输出单元表示动作 $a$ 的 $\hat{q}$ 函数。总的来说，$\hat{q}$ 可以被记为 $\hat{q}(s,\mathbf{w})\in\mathbb{R}^{|A|}$，简单起见，我们仍用 $\hat{q}(s,a,\mathbf{w})$ 来表示对 $(s,a)$ 的状态-行为值估计。

结构细节：输入为一张 $84\times 84\times 4$ 的图片；第一个卷积层有 32 个大小为 $8\times 8$、步幅（stride）为 $4$ 的滤波器，对输入图片进行卷积操作后，将结果输入非线性整流器（ReLU）[6]；第二个隐藏层有 64 个大小为 $4\times 4$、步幅为 $2$ 的滤波器，同样地，后面接非线性整流器；第三个隐藏层有 64 个大小为 $3\times 3$、步幅为 $1$ 的滤波器，同样地，后面接 ReLU；最后的隐藏层为包含 512 个整流器（ReLU）单元的全连接层（fully-connected layer）；输出层为全连接线性层。

# 图1

#### 7.2.2 原始像素预处理（Processing Raw Pixels）

原始的 Atari 2600 画面的大小为 $210\times 160\times 3$，最后的维度指的是 RGB 的通道数。[1] 中采用的预处理步骤的目的是降低输入的维度以及处理游戏模拟器的一些工件。我们将预处理过程总结如下：

$\bullet$ 单帧图片编码（single frame encoding）：为编码单帧图片，我们将返回所编码帧和上一帧每个像素颜色值的最大值，也就是说，我们返回两帧连续的原始图片的每个像素的最大值（pixel-wise nax-pooling）。

$\bullet$ 降维（dimensionality reduction）：从编码的 RGB 图像中提取 Y 通道（也称为亮度，luminance），将其重新缩放到 $(84\times 84\times 1)$。

对 4 帧最新的原始 RGB 图像应用上述处理过程，并将编码后的图片堆叠在一起，以生成 Q-网络的输入（大小为 $(84\times 84\times 1)$）。将最新的帧堆叠在一起来作为游戏状态也是将游戏环境转换为（近似）马尔科夫世界的一种方法。

#### 7.2.3 DQN 的训练算法（Training Algorithms for DQN）

由于没有理论性的保证，而且学习和训练往往很不稳定，因此过去常常避免使用大型的深度神经网络来近似状态-行为值函数。为使用大型非线性近似函数和大规模在线 Q-学习，DQN 引入了两个主要变化：使用经验回放（experience replay）和一个单独的目标网络（target network），算法1 展示了完整的算法。本质上，Q-网络是通过最小化以下均方误差来学习的：

$$
J(\mathbf{w})=\mathbb{E}_ {(s_t,a_t,r_t,s_{t+1})}[(y_t^{DQN}-\hat{q}(s_t,a_t,\mathbf{w}))^2]，
\tag{6}
$$

这里 $y_t^{DQN}$ 为提前一步学习目标（one-step ahead learning target）：
$$
y_t^{DQN}=r_t+\gamma\mathop{\max}_ {a'}\hat{q}(s_{t+1},a',\mathbf{w}^-)，
\tag{7}
$$

$\mathbf{w}^-$ 表示目标网络的参数，在线网络的参数 $\mathbf{w}$ 通过从曾经的状态转移 $(s_t,a_t,r_t,s_{t+1})$ 的小批量中采样梯度来更新。

（注意：）

## 参考文献

1. V. Mnih et al., "Human-level control through deep reinforcement learning," *Nature* 518(7540): 529-533, 2015.

2. H. van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double q-learning," *AAAI*, 2016.

3. Z. Wang et al., "Dueling network architectures for deep reinforcement learning," *arXiv preprint arXiv: 1511.06581*, 2015.

4. M. G. Bellemare et al., "The arcade learning environment: an evaluation platform for general agents," *IJCAI*, 2015.