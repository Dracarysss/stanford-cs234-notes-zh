# Lecture 6 CNNs and Deep Q-learning

# 课时6 卷积神经网络与深度 Q-学习 2018.03.20

## 7. 基于值的深度强化学习（Value-based Deep Reinforcement Learning）

这一部分我们介绍三种常见的基于值的深度强化学习（RL）算法：深度 Q-网络（Deep Q-network）[1]，双 DQN（Double DQN）[2] 和对抗 DQN（Dueling DQN）[3]。通过端到端的（end-to-end）强化学习，这三种神经网络架构都可以直接从像电子游戏的预处理像素这样的高维输入（high-dimensional inputs）中学习到成功的策略，它们在 Atari 2600 的49个游戏中达到了相当于专业的游戏测试员的水平 [4]。

这些架构都使用了卷积神经网络（Convolutional Neural Networks, CNNs）[5] 来从像素输入中提取特征。理解 CNNs 特征提取的机制有助于我们理解 DQN 的工作方式。 斯坦福大学 CS231N 课程网站提供了关于 CNNs 的详尽的介绍和例子，通过这个链接，读者可以阅读更多细节：http://cs231n.github.io/convolutional-networks/。本节的其余部分我们将关注 RL 和 基于值的深度 RL 算法的泛化。

### 7.1 概述：状态-行为值函数近似（Recap: Action-value Function Approximation）

上节课我们用参数化的近似函数来表示状态-行为值函数（Q-函数），若我们将参数记为 $\mathbf{w}$，那么这种设定中，Q-函数可以被表示为 $\hat{q}(s,a,\mathbf{w})$。

假设我们知道 $q(s,a)$，通过最小化真实状态-动作值函数 $q(s,a)$ 和近似估计之间的均方误差 $J(\mathbf{w})$，我们可以得到近似的 Q-函数：
$$
J(\mathbf{w})=\mathbb{E}[(q(s,a)-\hat{q}(s,a,\mathbf{w}))^2]。
\tah{1}
$$

我们可以通过随机梯度下降（stochastic gradient descent, SGD）找到 $J$ 的关于 $\mathbf{w}$ 的局部最小值，并且以下式来更新 $\mathbf{w}$：
$$
\Delta(\mathbf{w})=-\frac{1}{2}\alpha\nabla_{\mathbf{w}}J(\mathbf{w})=\alpha\mathbb{E}[(q(s,a)-\hat{q}(s,a,\mathbf{w}))\nabla_{\mathbf{w}}\hat{q}(s,a,\mathbf{w})]，
\tag{2}
$$
这里 $\alpha$ 为学习率（learning rate）。

## 参考文献

1. V. Mnih et al., "Human-level control through deep reinforcement learning," *Nature* 518(7540): 529-533, 2015.

2. H. van Hasselt, A. Guez, and D. Silver, "Deep reinforcement learning with double q-learning," *AAAI*, 2016.

3. Z. Wang et al., "Dueling network architectures for deep reinforcement learning," *arXiv preprint arXiv: 1511.06581*, 2015.

4. M. G. Bellemare et al., "The arcade learning environment: an evaluation platform for general agents," *IJCAI*, 2015.