# Lecture 10 Advanced Policy Gradient

# 课时10 高级策略梯度 2019.02.11

## 1. 策略梯度的目标（Policy Gradient Objective）

在策略梯度中，我们将策略 $\pi_\theta$ 参数化，并使用环境中的经历直接来优化它。我们首先定义基于当前策略 $\pi_\theta$ 的轨迹的概率为 $\pi_\theta(\tau)$：

$$
\pi_\theta(\tau) = \pi_\theta(s_1,a_1,...,s_T,a_T)=P(s_1)\prod_{t=1}^T \pi_\theta(a_t|s_t)P(s_{t+1}|s_t,a_t)，
$$

这里 $P(s_1)$ 为起始状态为 $s_1$ 的概率，$\pi_\theta(a_t|s_t)$ 为根据当前的策略在状态 $s_t$ 选择动作 $a_t$ 的概率，$P(s_{t+1}|s_t,a_t)$ 为在状态 $s_t$ 选择动作 $a_t$ 时，状态转移到 $s_{t+1}$ 的概率。注意，$\pi_\theta(\tau)$ 为轨迹的概率而 $\pi_\theta(a|s)$ 为给定状态时选择某个动作的概率。

和到目前为止我们讨论过的大多数其他 RL 目标类似，策略梯度的目标是最大化衰减奖励总和。

$$
\theta^* = \mathop{\arg\max}_{\theta} \mathbb{E} _{\tau\sim \pi _{\theta}(\tau)}[\sum_t \gamma^t r (s_t,a_t)]。
$$

我们将目标函数记为 $J(\theta)$，可以用蒙特卡洛方法估计 $J(\theta)$。我们用 $r(\tau)$ 来代表轨迹 $\tau$ 的衰减奖励总和。

$$
J(\theta) = \mathbb{E}_{\tau\sim \pi _{\theta}(\tau)}[\sum_t \gamma^t r (s_t,a_t)] = \int \pi _{\theta} (\tau) r(\tau) \text{d} \tau
$$

$$
 \approx \frac{1}{N}\sum_{i=1}^N \sum_{t=1}^T \gamma^t r(s_{i,t},a_{i,t})，
 $$

$$
\theta^* = \mathop{\arg\max}_\theta J(\theta)。
$$

我们定义 $P_\theta(s,a)$ 为 $(s,a)$ 出现在轨迹中的概率。注意，若时间步无穷大而且状态的平稳分布存在时，我们可以将 $P_\theta(s,a)$ 写为 $P_\theta(s,a)=d^{\pi_{\theta}}(s)\pi_{\theta}(a|s)$，这里 $d^{\pi_{\theta}}(s)$ 为遵照策略 $\pi_{\theta}$ 时的状态的平稳分布。

在无限时间步的情况下，我们有：
$$
\theta^{*} = \mathop{\arg\max}_{\theta}\sum _{t=1}^{\infty} \mathbb{E} _{(s,a) \sim P _{\theta}(s,a)}[\gamma^t r(s,a)]
$$

$$
= \mathop{\arg\max}_{\theta} \frac{1}{1-\gamma}\mathbb{E} _{(s,a) \sim P _{\theta}(s,a)}[r(s,a)]
$$

$$
= \mathop{\arg\max}_{\theta} \mathbb{E} _{(s,a) \sim P _{\theta}(s,a)}[r(s,a)]。
$$

在有限时间步的情况下，我们有：
$$
\theta^{*} = \mathop{\arg\max}_{\theta} \sum _{t=1}^{T} \mathbb{E} _{(s_t,a_t) \sim P _{\theta}(s_t,a_t)}[\gamma^t r(s_t,a_t)]。
$$

我们可以使用基于梯度的方法来完成上述优化，也就是说，我们需要找到 $J(\theta)$ 基于 $\theta$ 的梯度。
$$
\nabla_{theta}J(\theta) = \nabla_{\theta}\int \pi _{\theta} (\tau) r(\tau) \text{d} \tau
$$

$$
= \int \nabla_{\theta} \pi _{\theta} (\tau) r(\tau) \text{d} \tau
$$

$$
= \int \pi_{\theta}(\tau) \frac{\nabla_{theta} \pi_ {\theta} (\tau)}{\pi_{\theta}(\tau)}r(\tau)\text{d}\tau
$$

$$
= \mathbb{E}_ {\tau\sim\pi_{\theta}(\tau)}[\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)]
$$

通过对数导数技巧，我们将梯度从期望之外转移到了期望之内。这样做的好处就是，我们不再需要对状态转移函数求梯度，正如下面我们将看到的。
$$
\nabla_{theta}J(\theta) = \mathbb{E}_ {\tau\sim\pi_{\theta}(\tau)}[\nabla_{\theta}\log\pi_{\theta}(\tau)r(\tau)]
$$

$$
= \mathbb{E}_ {\tau\sim\pi_{\theta}(\tau)}[\nabla_{theta} [\logP(s_1)+\sum_{t=1}^{T}(\log\pi_{\theta}(a_t|s_t) + \log  P(s_{t+1}|s_t,a_t))]r(\tau)]
$$

$$
= \mathbb{E}_ {\tau\sim\pi_{\theta}